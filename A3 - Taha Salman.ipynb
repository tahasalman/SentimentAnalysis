{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Sentiment Analysis\n",
    "## Taha Salman (260721174)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB-vocab.txt file has been created!\n",
      "IMDB-test.txt file has ben created!\n",
      "IMDB-train.txt file has ben created!\n",
      "IMDB-valid.txt file has ben created!\n",
      "yelp-vocab.txt file has been created!\n",
      "yelp-test.txt file has ben created!\n",
      "yelp-train.txt file has ben created!\n",
      "yelp-valid.txt file has ben created!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def read_data(data_path,encoding='utf-8'):\n",
    "    data = []\n",
    "    with open(data_path,'r',encoding=encoding) as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "def pre_process_data(data):\n",
    "    '''\n",
    "    takes as input a list containing lines of data.\n",
    "    Removes punctuation marks, changes every word to lowercase, and then\n",
    "    returns a list containing a list of words and the last element is the class\n",
    "    '''\n",
    "    output = []\n",
    "    to_remove = string.punctuation\n",
    "    to_remove+=\"br\"\n",
    "    translator = str.maketrans(\"\",\"\",to_remove)\n",
    "    for line in data:\n",
    "        word_list = line.split(\" \")\n",
    "        final_word_list = []\n",
    "        num_words = len(word_list)-1\n",
    "        for i in range(0,num_words):\n",
    "            word = word_list[i]\n",
    "            word = word.translate(translator)\n",
    "            word = word.lower()\n",
    "            if word:\n",
    "                final_word_list.append(word)\n",
    "\n",
    "        last_words = word_list[-1].split('\\t')\n",
    "        final_word_list.append(last_words[0])\n",
    "        final_word_list.append(last_words[1].strip('\\n'))\n",
    "\n",
    "        output.append(final_word_list)\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab_dict = {}\n",
    "    for line in data:\n",
    "        line_length = len(line)-1\n",
    "        for i in range(0,line_length):\n",
    "            if line[i] in vocab_dict:\n",
    "                vocab_dict[line[i]] = vocab_dict[line[i]] + 1\n",
    "            else:\n",
    "                vocab_dict[line[i]] = 1\n",
    "\n",
    "    sorted_list = []\n",
    "\n",
    "    for word in sorted(vocab_dict, key=vocab_dict.get,reverse=True):\n",
    "        sorted_list.append((word,vocab_dict[word]))\n",
    "    return sorted_list\n",
    "\n",
    "\n",
    "def save_vocab_file(vocab_list,saving_path,encoding='utf-8'):\n",
    "    output = \"\"\n",
    "\n",
    "    for i in range(0,len(vocab_list)):\n",
    "        output = output + vocab_list[i][0] + \"\\t\"\n",
    "        output = output + str(i+1) + \"\\t\"\n",
    "        output = output + str(vocab_list[i][1]) + \"\\n\"\n",
    "\n",
    "    with open(saving_path,'w',encoding=encoding) as f:\n",
    "        f.write(output)\n",
    "\n",
    "\n",
    "def create_vocab_file(vocab_size,reading_path,saving_path):\n",
    "    vocab_size = 10000\n",
    "\n",
    "    data = read_data(reading_path)\n",
    "    processed_data = pre_process_data(data)\n",
    "\n",
    "    vocab_list = build_vocab(processed_data)[0:vocab_size]\n",
    "    save_vocab_file(vocab_list, saving_path)\n",
    "\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    vocab_dict = {}\n",
    "    data = []\n",
    "    with open(vocab_path,\"r\",encoding=\"utf-8\") as f:\n",
    "        data = f.readlines()\n",
    "    for line in data:\n",
    "        temp = line.split(\"\\t\")\n",
    "        vocab_dict[temp[0]] = temp[1]\n",
    "    return vocab_dict\n",
    "\n",
    "\n",
    "def code_data(data,vocab):\n",
    "    output = \"\"\n",
    "    for line in data:\n",
    "        num_words = len(line) - 2\n",
    "        for i in range(0,num_words):\n",
    "            if line[i] in vocab:\n",
    "                output += vocab[line[i]] + \" \"\n",
    "\n",
    "        output += line[-2] + \"\\t\"\n",
    "        output += line[-1] + \"\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def create_coded_file(reading_path,saving_path,vocab_path):\n",
    "    data = read_data(reading_path)\n",
    "    processed_data = pre_process_data(data)\n",
    "    vocab_dict = read_vocab(vocab_path)\n",
    "    coded_data = code_data(processed_data,vocab_dict)\n",
    "    with open(saving_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(coded_data)\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    datasets = (\"IMDB\",\"yelp\")\n",
    "    dataclasses= (\"test\",\"train\",\"valid\")\n",
    "\n",
    "    for dataset in datasets:\n",
    "        create_vocab_file(\n",
    "            vocab_size=10000,\n",
    "            reading_path='Data/Raw/{}-train.txt'.format(dataset),\n",
    "            saving_path='Data/Processed/{}-vocab.txt'.format(dataset)\n",
    "        )\n",
    "        print(\"{}-vocab.txt file has been created!\".format(dataset))\n",
    "\n",
    "        for dataclass in dataclasses:\n",
    "            create_coded_file(\n",
    "                reading_path=\"Data/Raw/{}-{}.txt\".format(dataset,dataclass),\n",
    "                saving_path=\"Data/Processed/{}-{}.txt\".format(dataset,dataclass),\n",
    "                vocab_path=\"Data/Processed/{}-vocab.txt\".format(dataset)\n",
    "            )\n",
    "            print(\"{}-{}.txt file has ben created!\".format(dataset,dataclass))\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    prepare_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Random Classifier with training data from the path Data/Processed/yelp-train.txt\n",
      "There are 5 possible classes:\n",
      "Class 5\n",
      "Class 3\n",
      "Class 2\n",
      "Class 1\n",
      "Class 4\n",
      "Making Predictions on the data from the path Data/Processed/yelp-test.txt\n",
      "The F1 score for this random classifier is 0.2105\n",
      "\n",
      "Initializing Majority Class Classifier with training data from the path Data/Processed/yelp-train.txt\n",
      "There are 5 possible classes:\n",
      "Class 5\n",
      "Class 3\n",
      "Class 2\n",
      "Class 1\n",
      "Class 4\n",
      "Making Predictions on the data from the path Data/Processed/yelp-test.txt\n",
      "The F1 score for this Majority Class Classifier is 0.351\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def read_data(data_path, encoding=\"utf-8\"):\n",
    "    data = []\n",
    "    with open(data_path, \"r\", encoding=encoding) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    return data\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self,training_data_path,classes=[]):\n",
    "        if classes:\n",
    "            self.classes = classes\n",
    "        else:\n",
    "            self.classes = self.set_classes(training_data_path)\n",
    "\n",
    "    def set_classes(self,data_path):\n",
    "        classes = []\n",
    "        data = read_data(data_path)\n",
    "        for line in data:\n",
    "            temp = line.split(\" \")\n",
    "            cl = (temp[-1].split(\"\\t\")[1]).strip('\\n')\n",
    "            if cl not in classes:\n",
    "                classes.append(cl)\n",
    "        return classes\n",
    "\n",
    "\n",
    "class RandomClassifier(Classifier):\n",
    "\n",
    "    def classify(self,data_path):\n",
    "        data = read_data(data_path)\n",
    "        predictions = []\n",
    "        for line in data:\n",
    "            predictions.append(self.predict_class())\n",
    "        return predictions\n",
    "\n",
    "    def predict_class(self):\n",
    "        rint = random.randint(0,len(self.classes)-1)\n",
    "        return self.classes[rint]\n",
    "\n",
    "\n",
    "\n",
    "class MajorityClassClassifier(Classifier):\n",
    "    def __init__(self, training_data_path, classes=[]):\n",
    "        super().__init__(training_data_path, classes)\n",
    "        self.class_frequencies = self.set_class_frequencies(training_data_path)\n",
    "\n",
    "    def classify(self,data_path):\n",
    "        predictions = []\n",
    "        data = read_data(data_path)\n",
    "        for line in data:\n",
    "            predictions.append(self.predict_class())\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def set_class_frequencies(self,data_path):\n",
    "        class_dict = {}\n",
    "        data = read_data(data_path)\n",
    "\n",
    "        for line in data:\n",
    "            temp = line.split(\" \")\n",
    "            cl = (temp[-1].split(\"\\t\")[1]).strip('\\n')\n",
    "            if cl in class_dict:\n",
    "                class_dict[cl] = class_dict[cl] + 1\n",
    "            else:\n",
    "                class_dict[cl] = 1\n",
    "\n",
    "        class_frequencies = []\n",
    "        for cl in sorted(class_dict,key=class_dict.get,reverse=True):\n",
    "            class_frequencies.append((cl,class_dict[cl]))\n",
    "\n",
    "        return class_frequencies\n",
    "\n",
    "    def predict_class(self):\n",
    "        return self.class_frequencies[0][0]\n",
    "\n",
    "\n",
    "class PerformanceTester():\n",
    "    def __init__(self,predictions):\n",
    "        self.predictions=predictions\n",
    "\n",
    "    def get_F1_score(self,data_path):\n",
    "        results = PerformanceTester.get_actual_results(data_path)\n",
    "        score = f1_score(\n",
    "            y_true=results,\n",
    "            y_pred=self.predictions,\n",
    "            average=\"micro\"\n",
    "        )\n",
    "        return score\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_actual_results(data_path):\n",
    "        results = []\n",
    "        data = read_data(data_path)\n",
    "        for line in data:\n",
    "            temp = line.split(\" \")\n",
    "            cl = (temp[-1].split(\"\\t\")[1]).strip('\\n')\n",
    "            results.append(cl)\n",
    "        return results\n",
    "\n",
    "\n",
    "def run_random_classifier():\n",
    "    TRAINING_DATA_PATH = \"Data/Processed/yelp-train.txt\"\n",
    "    TESTING_DATA_PATH = \"Data/Processed/yelp-test.txt\"\n",
    "\n",
    "    print(\"Initializing Random Classifier with training data from the path {}\".format(TRAINING_DATA_PATH))\n",
    "    rc = RandomClassifier(TRAINING_DATA_PATH)\n",
    "    cls = rc.classes\n",
    "    print(\"There are {} possible classes:\".format(len(cls)))\n",
    "    for i in range(0, len(cls)):\n",
    "        print(\"Class {}\".format(cls[i]))\n",
    "\n",
    "    print(\"Making Predictions on the data from the path {}\".format(TESTING_DATA_PATH))\n",
    "    predictions = rc.classify(TESTING_DATA_PATH)\n",
    "\n",
    "    p_tester = PerformanceTester(predictions)\n",
    "    f1s = p_tester.get_F1_score(TESTING_DATA_PATH)\n",
    "    print(\"The F1 score for this random classifier is {}\".format(f1s))\n",
    "\n",
    "\n",
    "def run_majority_class_classifier():\n",
    "    TRAINING_DATA_PATH = \"Data/Processed/yelp-train.txt\"\n",
    "    TESTING_DATA_PATH = \"Data/Processed/yelp-test.txt\"\n",
    "\n",
    "    print(\"Initializing Majority Class Classifier with training data from the path {}\".format(TRAINING_DATA_PATH))\n",
    "    mcc = MajorityClassClassifier(TRAINING_DATA_PATH)\n",
    "    cls = mcc.classes\n",
    "    print(\"There are {} possible classes:\".format(len(cls)))\n",
    "    for i in range(0, len(cls)):\n",
    "        print(\"Class {}\".format(cls[i]))\n",
    "\n",
    "    print(\"Making Predictions on the data from the path {}\".format(TESTING_DATA_PATH))\n",
    "    predictions = mcc.classify(TESTING_DATA_PATH)\n",
    "\n",
    "    p_tester = PerformanceTester(predictions)\n",
    "    f1s = p_tester.get_F1_score(TESTING_DATA_PATH)\n",
    "    print(\"The F1 score for this Majority Class Classifier is {}\".format(f1s))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_random_classifier()\n",
    "    print()\n",
    "    run_majority_class_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
